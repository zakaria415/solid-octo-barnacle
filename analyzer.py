[  ] # analyzer.py - Ù…Ù†Ø·Ù‚ Ø¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ ÙˆØ§Ù„Ø­ÙØ¸
[  ] 
[  ] # Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
[  ] from datetime import datetime
[  ] from db_helpers import (
[  ]     get_db, 
[  ]     create_analysis_run,
[  ]     update_analysis_run_status, 
[  ]     create_review, 
[  ]     create_sales_opportunity,
[  ]     get_review_stats
[  ] )
[  ] from models import AnalysisRun # Ù†Ø³ØªÙˆØ±Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ø¹Ø±Ø¶
[  ] # Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¹Ù…Ù„ (ÙŠØ¬Ø¨ ØªØ«Ø¨ÙŠØªÙ‡Ø§: pip install requests beautifulsoup4 textblob)
[  ] import requests
[  ] from bs4 import BeautifulSoup
[  ] from textblob import TextBlob
[  ] import re 
[  ] import time
[  ] # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª (ÙŠÙØªØ±Ø¶ Ø£Ù† Ù…Ù„Ù notifier.py Ù…ÙˆØ¬ÙˆØ¯)
[  ] from notifier import send_analysis_notification 
[  ] 
[  ] # ----------------- 1. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¹Ù„ÙŠØ© -----------------
[  ] 
[  ] def scrape_reviews(url: str, review_selector: str) -> list[dict]:
[  ]     """
[  ]     Ø§Ø³ØªØ®Ù„Ø§Øµ Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©.
[  ]     *** Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ­ØªØ§Ø¬ Ù„Ù„ØªØ®ØµÙŠØµ Ø§Ù„ÙƒØ§Ù…Ù„ Ø­Ø³Ø¨ Ù‡ÙŠÙƒÙ„ HTML Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù. ***
[  ]     """
[  ]     reviews_list = []
[  ]     
[  ]     try:
[  ]         response = requests.get(url, timeout=15)
[  ]         response.raise_for_status() # Ø¥Ø·Ù„Ø§Ù‚ Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù„Ø£Ø®Ø·Ø§Ø¡ HTTP
[  ]         
[  ]         soup = BeautifulSoup(response.content, 'html.parser')
[  ]         
[  ]         review_elements = soup.select(review_selector) 
[  ]         
[  ]         for element in review_elements:
[  ]             # ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙØ­Ø¯Ù‘Ø¯Ø§Øª (.review-title, .review-body, Ø¥Ù„Ø®) Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙˆÙ‚Ø¹
[  ]             title = element.select_one('.review-title')?.text.strip()
[  ]             text = element.select_one('.review-body')?.text.strip()
[  ]             rating_str = element.select_one('.review-rating')?.text.strip() 
[  ]             
[  ]             if text:
[  ]                 # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„Ù„ØªÙ‚ÙŠÙŠÙ…
[  ]                 rating = re.search(r'\d+', rating_str).group(0) if rating_str and re.search(r'\d+', rating_str) else None
[  ] 
[  ]                 reviews_list.append({
[  ]                     'title': title,
[  ]                     'review_text': text,
[  ]                     'rating': rating,
[  ]                     # Ø­Ù‚ÙˆÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØªÙ… Ù…Ù„Ø¤Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
[  ]                 })
[  ] 
[  ]     except requests.exceptions.RequestException as e:
[  ]         print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ù…Ù† {url}: {e}")
[  ]     
[  ]     return reviews_list
[  ] 
[  ] def analyze_sentiment(text: str) -> tuple[str, float, float, str]:
[  ]     """
[  ]     ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø°Ø§ØªÙŠØ© ÙˆØ§Ù„Ù„ØºØ©.
[  ]     *** Ù…Ù„Ø§Ø­Ø¸Ø©: TextBlob Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ ÙˆÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù…ÙƒØªØ¨Ø© Ø¹Ø±Ø¨ÙŠØ© Ù…ØªØ®ØµØµØ©. ***
[  ]     """
[  ]     try:
[  ]         # Ø§Ø³ØªØ®Ø¯Ø§Ù… TextBlob (ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ø¨ÙŠ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬)
[  ]         analysis = TextBlob(text)
[  ]         
[  ]         sentiment_score = analysis.sentiment.polarity # Ù…Ù† -1 (Ø³Ù„Ø¨ÙŠ) Ø¥Ù„Ù‰ +1 (Ø¥ÙŠØ¬Ø§Ø¨ÙŠ)
[  ]         subjectivity_score = analysis.sentiment.subjectivity
[  ]         language = 'ar' 
[  ]         
[  ]         if sentiment_score > 0.3:
[  ]             label = 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ'
[  ]         elif sentiment_score < -0.1:
[  ]             label = 'Ø³Ù„Ø¨ÙŠ'
[  ]         else:
[  ]             label = 'Ù…Ø­Ø§ÙŠØ¯'
[  ]             
[  ]         return label, sentiment_score, subjectivity_score, language
[  ] 
[  ]     except Exception as e:
[  ]         print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {e}")
[  ]         return 'Ù…Ø­Ø§ÙŠØ¯', 0.0, 0.5, 'unknown'
[  ] 
[  ] def find_sales_intent(text: str) -> tuple[bool, str | None]:
[  ]     """
[  ]     ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ÙØ±ØµØ© Ù…Ø¨ÙŠØ¹Ø§Øª Ø£Ùˆ ØªØ­Ø³ÙŠÙ† (ÙØ±ØµØ©).
[  ]     """
[  ]     keywords = ["Ø£ØªÙ…Ù†Ù‰ Ù„Ùˆ", "ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰", "ÙŠØ¬Ø¨ Ø£Ù†", "Ù„Ùˆ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ", "Ù†Ù‚Øµ ÙÙŠ"]
[  ]     
[  ]     if any(k in text.lower() for k in keywords):
[  ]         product_title = f"Ø·Ù„Ø¨ Ù…ÙŠØ²Ø©/ØªØ­Ø³ÙŠÙ†: {text[:30].replace(text[:30].split()[0], '')}..."
[  ]         return True, product_title
[  ]         
[  ]     return False, None
[  ] 
[  ] # ----------------- 2. ÙˆØ¸Ø§Ø¦Ù Ø¥Ø¯Ø§Ø±Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ -----------------
[  ] 
[  ] def start_new_analysis_run(target_site: str, start_url: str) -> AnalysisRun | None:
[  ]     """
[  ]     ØªØ¨Ø¯Ø£ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¬Ù„ AnalysisRun ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø­Ø§Ù„Ø© 'pending'.
[  ]     """
[  ]     run_data = {
[  ]         "target_site": target_site,
[  ]         "start_url": start_url,
[  ]         "status": "pending",  
[  ]         "created_at": datetime.utcnow()
[  ]     }
[  ] 
[  ]     with get_db() as db:
[  ]         new_run = create_analysis_run(db, run_data)
[  ]     
[  ]     if new_run:
[  ]         print(f"âœ… ØªÙ… Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…ÙˆÙ‚Ø¹: {target_site} Ø¨Ø§Ù„Ù…Ø¹Ø±Ù‘Ù (ID): {new_run.id}")
[  ]     else:
[  ]         print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„Ù„Ù…ÙˆÙ‚Ø¹: {target_site}")
[  ] 
[  ]     return new_run
[  ] 
[  ] 
[  ] def process_analysis_run(run_to_process: AnalysisRun):
[  ]     """
[  ]     Ø¥Ø¯Ø§Ø±Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„: Ø§Ù„ØªØ­Ø¯ÙŠØ«ØŒ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§ØµØŒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ØŒ Ø§Ù„Ø­ÙØ¸ØŒ ÙˆØ§Ù„Ø¥Ø´Ø¹Ø§Ø±.
[  ]     """
[  ]     run_id = run_to_process.id
[  ]     start_url = run_to_process.start_url
[  ]     
[  ]     print(f"\n--- Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ ID: {run_id} Ù„Ù„Ù…ÙˆÙ‚Ø¹: {run_to_process.target_site} ---")
[  ]     
[  ]     # 1. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ø¥Ù„Ù‰ "running" (Ù‚ÙŠØ¯ Ø§Ù„ØªØ´ØºÙŠÙ„)
[  ]     with get_db() as db:
[  ]         updated_run = update_analysis_run_status(db, run_id, "running")
[  ]         if not updated_run:
[  ]             print(f"âŒ ÙØ´Ù„ ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ ID: {run_id} Ø¥Ù„Ù‰ 'running'.")
[  ]             return
[  ] 
[  ]     print(f"ğŸ”„ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ø¥Ù„Ù‰: {updated_run.status}")
[  ] 
[  ]     try:
[  ]         # 2. Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ¹Ù„ÙŠ
[  ]         review_selector = 'div.review-card' # ÙŠØ¬Ø¨ ØªØ®ØµÙŠØµÙ‡
[  ]         raw_reviews = scrape_reviews(start_url, review_selector)
[  ]         
[  ]         # 3. ØªØ­Ù„ÙŠÙ„ ÙˆØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
[  ]         reviews_to_save = []
[  ]         opportunities_to_save = []
[  ]         
[  ]         for r_data in raw_reviews:
[  ]             # Ø§Ù„ØªØ­Ù„ÙŠÙ„
[  ]             label, score, subjectivity, lang = analyze_sentiment(r_data['review_text'])
[  ]             is_opportunity, op_title = find_sales_intent(r_data['review_text'])
[  ]             
[  ]             review_final_data = {
[  ]                 'analysis_run_id': run_id,
[  ]                 'title': r_data.get('title'),
[  ]                 'review_text': r_data['review_text'],
[  ]                 'rating': r_data.get('rating'),
[  ]                 'sentiment_label': label,
[  ]                 'compound_score': score,
[  ]                 'subjectivity': subjectivity,
[  ]                 'language': lang,
[  ]                 'has_sales_intent': is_opportunity,
[  ]                 'scraped_at': datetime.utcnow()
[  ]             }
[  ]             reviews_to_save.append(review_final_data)
[  ] 
[  ]             if is_opportunity:
[  ]                 opportunities_to_save.append({
[  ]                     'analysis_run_id': run_id,
[  ]                     'product_title': op_title,
[  ]                     'review_text': r_data['review_text'],
[  ]                     'compound_score': score,
[  ]                     'estimated_value': 50.0, # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
[  ]                     'status': 'pending',
[  ]                     'created_at': datetime.utcnow(),
[  ]                 })
[  ]             
[  ]         # 4. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
[  ]         total_reviews = len(reviews_to_save)
[  ]         positive_count = sum(1 for r in reviews_to_save if r['sentiment_label'] == 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ')
[  ]         negative_count = sum(1 for r in reviews_to_save if r['sentiment_label'] == 'Ø³Ù„Ø¨ÙŠ')
[  ]         neutral_count = total_reviews - (positive_count + negative_count)
[  ]         avg_compound_score = sum(r['compound_score'] for r in reviews_to_save) / total_reviews if total_reviews else 0
[  ]         positive_perc = (positive_count / total_reviews) * 100 if total_reviews else 0
[  ] 
[  ]         with get_db() as db:
[  ]             # Ø­ÙØ¸ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙˆÙØ±Øµ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª
[  ]             for r_data in reviews_to_save:
[  ]                 create_review(db, r_data)
[  ]             for o_data in opportunities_to_save:
[  ]                 create_sales_opportunity(db, o_data)
[  ]             
[  ]             # 5. ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù€ "completed" ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬
[  ]             final_data = {
[  ]                 "total_reviews": total_reviews,
[  ]                 "positive_count": positive_count,
[  ]                 "negative_count": negative_count,
[  ]                 "neutral_count": neutral_count,
[  ]                 "positive_percentage": positive_perc,
[  ]                 "avg_compound_score": avg_compound_score,
[  ]                 "completed_at": datetime.utcnow(),
[  ]                 "status": "completed"
[  ]             }
[  ]             
[  ]             db.query(AnalysisRun).filter(AnalysisRun.id == run_id).update(final_data)
[  ]             db.commit()
[  ]             
[  ]             print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ID: {run_id}. Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„ÙƒÙ„ÙŠØ©: {total_reviews}")
[  ]             
[  ]             # 6. Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„Ù†Ø¬Ø§Ø­
[  ]             send_analysis_notification(run_to_process, is_success=True)
[  ] 
[  ]     except Exception as e:
[  ]         error_msg = str(e)
[  ]         print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ID {run_id}: {error_msg}")
[  ]         
[  ]         # 7. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ø¥Ù„Ù‰ "failed" ÙˆØ¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„ÙØ´Ù„
[  ]         with get_db() as db:
[  ]             run_to_process.status = "failed"
[  ]             db.add(run_to_process)
[  ]             db.commit()
[  ]             
[  ]             send_analysis_notification(run_to_process, is_success=False, error_message=error_msg)
[  ]             
[  ] # ----------------------------------------------------
[  ] # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° (ÙŠØ¬Ø¨ Ø±Ø¨Ø·Ù‡ Ø¨Ù€ Streamlit)
[  ] # ----------------------------------------------------
[  ] if __name__ == "__main__":
[  ]     
[  ]     # 1. Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
[  ]     run_obj = start_new_analysis_run("Ù…Ù†ØµØ© ØªØ¬Ø±ÙŠØ¨ÙŠØ© X", "http://test.com/reviews")
[  ]     
[  ]     if run_obj:
[  ]         # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (ÙˆÙ‡ÙŠ Ø¹Ù…Ù„ÙŠØ© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰)
[  ]         # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ StreamlitØŒ ÙŠØ¬Ø¨ ØªØ´ØºÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ÙÙŠ Ù…Ø¤Ø´Ø± ØªØ±Ø§Ø¨Ø· (Thread) Ø£Ùˆ Ø¹Ù…Ù„ÙŠØ© Ù…Ù†ÙØµÙ„Ø©
[  ]         # Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
[  ]         process_analysis_run(run_obj)
[  ]         
[  ]         # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
[  ]         with get_db() as db:
[  ]             final_run = db.query(AnalysisRun).filter(AnalysisRun.id == run_obj.id).first()
[  ]             if final_run:
[  ]                 print(f"\n*** ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ (ID: {final_run.id}) ***")
[  ]                 print(f"Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {final_run.status}")
[  ]                 print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª: {final_run.total_reviews}")
[  ]