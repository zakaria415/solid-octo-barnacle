# scraper_core.py (Final Version with Dynamic Scraping)

import aiohttp
import asyncio
import re
import nltk
from bs4 import BeautifulSoup
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from db_helpers import save_analysis_result 
from urllib.parse import urljoin # Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªÙŠØ±Ø§Ø¯ urljoin
from app import load_config # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† load_config Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ app.py

# --- Initialization ---
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except nltk.downloader.DownloadError:
    nltk.download('vader_lexicon')

analyzer = SentimentIntensityAnalyzer()

# --- Core Asynchronous Functions ---

async def fetch_page(session, url):
    """Fetches content from a single URL."""
    try:
        async with session.get(url, timeout=10) as response:
            response.raise_for_status()  
            return await response.text()
    except aiohttp.ClientError as e:
        print(f"Error fetching {url}: {e}")
        return None
    except asyncio.TimeoutError:
        print(f"Timeout occurred while fetching {url}.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while fetching {url}: {e}")
        return None

def extract_text(html_content):
    """Extracts clean, non-script/style text content from HTML."""
    if not html_content:
        return ""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose() 
        text = soup.get_text()
        cleaned_text = re.sub(r'\s+', ' ', text).strip()
        return cleaned_text
    except Exception as e:
        print(f"Error during text extraction: {e}")
        return ""

def analyze_sentiment(text):
    """Analyzes the sentiment of a given text using VADER."""
    if not text:
        return 0.0 
    score = analyzer.polarity_scores(text)
    return score['compound'] 

# --- NEW: Dynamic Link Discovery Function ---
def get_next_page_url(html_content, current_url):
    """
    ÙŠØ³ØªØ®Ø±Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙØ­Ø¯Ø¯ CSS ÙÙŠ config.json.
    """
    if not html_content:
        return None
    
    try:
        config = load_config() # ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª config.json
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù€ next_page
        next_page_selector = config.get('selectors', {}).get('next_page')
        base_url = config.get('base_url')
        
        if not next_page_selector or not base_url:
            return None
            
        soup = BeautifulSoup(html_content, 'html.parser')
        next_page_link = soup.select_one(next_page_selector)
        
        if next_page_link and 'href' in next_page_link.attrs:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… urljoin Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
            next_url = urljoin(base_url, next_page_link['href'])
            return next_url
            
    except Exception as e:
        # print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† {current_url}: {e}") # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        return None

# --- NEW: Orchestration Function with Dynamic Crawling ---
def run_scraper_and_analysis(db_url, start_url, pages_to_scrape, concurrency):
    """
    Orchestrates the scraping and sentiment analysis process using a dynamic queue.
    """
    print(f"\n--- Starting Scraping and Analysis (Dynamic) ---")
    print(f"Target URL: {start_url}")
    print(f"Pages to scrape limit: {pages_to_scrape}")
    print(f"Concurrency limit: {concurrency}")

    urls_queue = asyncio.Queue()
    urls_queue.put_nowait(start_url)
    scraped_urls = {start_url} # Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø²Ø­Ù Ù†ÙØ³ Ø§Ù„Ø±Ø§Ø¨Ø·
    all_results = [] 
    
    try:
        loop = asyncio.get_event_loop()
        
        async def scraper_worker(session):
            nonlocal all_results
            pages_counter = 0
            
            # Ø§Ù„Ø­Ù„Ù‚Ø© ØªØ³ØªÙ…Ø± Ù…Ø§ Ø¯Ø§Ù…Øª Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„Ø·Ø§Ø¨ÙˆØ± Ù„Ù… ØªÙ†ØªÙ‡Ù ÙˆÙ„Ù… ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
            while pages_counter < pages_to_scrape and not urls_queue.empty():
                try:
                    url = await asyncio.wait_for(urls_queue.get(), timeout=1) # timeout Ù„Ù„Ø­Ù…Ø§ÙŠØ©
                    if url is None: continue
                    
                    print(f"ğŸŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø²Ø­Ù Ø¥Ù„Ù‰: {url} (Ø§Ù„ØµÙØ­Ø©: {pages_counter + 1} Ù…Ù† {pages_to_scrape})")
                    html_content = await fetch_page(session, url)
                    
                    if html_content:
                        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±
                        extracted_text = extract_text(html_content)
                        sentiment_score = analyze_sentiment(extracted_text)
                        
                        all_results.append({
                            'url': url,
                            'sentiment_score': sentiment_score,
                            'text_content': extracted_text
                        })
                        
                        # 2. Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ¥Ø¶Ø§ÙØªÙ‡Ø§ Ù„Ù„Ø·Ø§Ø¨ÙˆØ±
                        next_url = get_next_page_url(html_content, url)
                        if next_url and next_url not in scraped_urls:
                            urls_queue.put_nowait(next_url)
                            scraped_urls.add(next_url)
                            print(f"-> ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ¥Ø¶Ø§ÙØªÙ‡: {next_url}")

                        pages_counter += 1
                    
                    urls_queue.task_done()
                    
                except asyncio.TimeoutError:
                    # Ù‚Ø¯ ÙŠØ­Ø¯Ø« Ø¹Ù†Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ø±Ø§Ø¨Ø· Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
                    print("âŒ› Ø§Ù†ØªÙ‡Ù‰ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„.")
                    break 
                except Exception as worker_error:
                    print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Worker Ø¹Ù†Ø¯ {url}: {worker_error}")
                    urls_queue.task_done()

        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        async def main_scraper_task():
            async with aiohttp.ClientSession() as session:
                # Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…Ø§Ù„ Ø¨Ø¹Ø¯Ø¯ ÙŠØ³Ø§ÙˆÙŠ Ù‚ÙŠÙ…Ø© Ø§Ù„ØªØ²Ø§Ù…Ù† (concurrency)
                workers = [asyncio.create_task(scraper_worker(session)) for _ in range(concurrency)]
                await urls_queue.join()
                
                # Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø¨Ù…Ø¬Ø±Ø¯ Ø¥ÙØ±Ø§Øº Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
                for worker in workers:
                    worker.cancel()

        loop.run_until_complete(main_scraper_task())

        print(f"\nâœ… Finished dynamic scraping. Found {len(all_results)} analysis results.")

        # Process results and save to database
        for result in all_results:
            try:
                save_analysis_result(
                    db_url, 
                    result['url'], 
                    result['sentiment_score'], 
                    result['text_content']
                )
                # print(f"-> Saved analysis for {result['url']}")
            except Exception as db_error:
                print(f"âš ï¸ Error saving analysis for {result['url']}: {db_error}")

    except Exception as e:
        print(f"âŒ An error occurred during the overall process: {e}")

    print("--- Process Finished ---\n")